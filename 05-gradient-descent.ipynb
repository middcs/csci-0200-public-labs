{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 5: Analyzing Gradient Descent\n",
    "\n",
    "- [Your name]\n",
    "- [The name of one of your group members]\n",
    "- [The name of your other group member]\n",
    "\n",
    "**Learning Objectives**:\n",
    "\n",
    "- You will use recurrence relations to prove an algorithm used for training models in machine learning and artificial intelligence. \n",
    "- You will describe the *rate of convergence* for this algorithm. \n",
    "- You will perform simulations and interpret your results in the context of the theory you have developed. \n",
    "\n",
    "**Assessment**: \n",
    "\n",
    "Labs are assessed according to [the posted EMRN rubric](https://www.philchodrow.prof/CSCI-0200/pages/lab-assessment.html).\n",
    "\n",
    "# Collaboration Statement\n",
    "\n",
    "[*Please describe the contributions of each group member. Please also describe the support you received from Course Assistants, other classmates, and any online resources such as StackExchange or ChatGPT.*]\n",
    "\n",
    "# Instructions\n",
    "\n",
    "In Parts A-C, **you should not be typing during the lab period**. You can use your device to look at the lab instructions and for quick checks, but now is not the time for typing. Instead, you should work with your group at your board or on your mini-boards on the specified problems. \n",
    "\n",
    "- Remember your group norms from Lab 1. \n",
    "- Pass the marker often, *at minimum* twice in every Part. \n",
    "- Every group member should take a picture before erasing anything. \n",
    "- For problems that require you to submit an image of a whiteboard, if you are working on those problems after the lab period, it's also acceptable to submit a hand-drawn image on paper or a tablet. "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Background: Gradient Descent\n",
    "\n",
    "In many contexts in data science, machine learning, artificial intelligence, and various forms of engineering, it is necessary to *minimize a function* -- find it's smallest value. If you've ever wondered why OpenAI needs all that money, the answer is that they need to use supercomputers in order to minimize extremely complicated functions very efficiently. \n",
    "\n",
    "\n",
    "\n",
    "**Definition**: A function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ has a (strict) *minimizer* $x^*\\in \\mathbb{R}$ if, for all $x \\in \\mathbb{R}$ such that $x \\neq x^*$, it is the case that $f(x^*) < f(x)$. In this case, the *minimum value* of $f$ is $f(x^*)$. The task of *minimizing a function* is the task of finding a minimum $x^*$, if it exists. \n",
    "\n",
    "For example, the function $f(x) = \\frac{1}{2}x^2$ has a strict minimizer at $x^* = 0$, with minimum value $0$. Here's how that looks on a graph: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "import numpy as np \n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "\n",
    "x = np.linspace(-1, 1, 1001)\n",
    "y = 1/2*x**2 \n",
    "plt.plot(x, y, label = r\"$f(x) = \\frac{1}{2}x^2$\")\n",
    "plt.scatter([0], [0], color = \"black\", label = r\"($x^*, f(x^*)$)\", zorder = 10)\n",
    "plt.legend()\n",
    "labs = plt.gca().set(xlabel = r\"$x$\", ylabel = r\"$y = f(x) = \\frac{1}{2}x^2$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In all real-world applications, we have to deal with more complicated functions whose minimizers we don't know exactly. To do this, we use an algorithm called *gradient descent*, in which we take a small step in (what is hopefully) the right direction many times over. For example, here's an example of gradient descent in action on this function. We start off with an initial guess that might be very far from the correct minimizer. Then, we take little steps in the right direction, towards the basin of the parabola. As we go, we get closer and closer to the correct minimizer at $x = 0$. It's easiest to start with a picture: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the plot\n",
    "plt.plot(x, y, label = r\"$f(x) = x^2$\")\n",
    "\n",
    "# begin with an initial guess about the location of the minimizer\n",
    "x_guess = 0.9\n",
    "\n",
    "# plot the guess\n",
    "plt.scatter([x_guess], [1/2*x_guess**2], color = \"black\", zorder = 10, s = 30)\n",
    "\n",
    "\n",
    "T = 100     # number of iterations\n",
    "t = 0       # initial timestep\n",
    "alpha = 0.1 # learning rate\n",
    "\n",
    "while t < T: \n",
    "    \n",
    "    # plot current guess\n",
    "    plt.scatter([x_guess], [1/2*x_guess**2], color = \"black\", zorder = 10, s = 5, alpha = 0.5)\n",
    "    \n",
    "    # adjust the guess\n",
    "    x_guess = x_guess - alpha*x_guess\n",
    "    \n",
    "    # increment the timestep\n",
    "    t = t+1\n",
    "\n",
    "labs = plt.gca().set(xlabel = r\"$x$\", ylabel = r\"$y = f(x) = \\frac{1}{2}x^2$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In this lab, we are going to formally state that the gradient descent algorithm works (successfully comes close to the minimizer) and describe how quickly this occurs. We're going to focus our attention on the function $f(x) = \\frac{1}{2}x^2$, although we'll consider more general quadratic functions in the last part of the lab. "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The Gradient Descent Algorithm For $f(x) = \\frac{1}{2}x^2$\n",
    "\n",
    "The algorithm for gradient descent for $f(x) = \\frac{1}{2}x^2$ works as follows. \n",
    "\n",
    "**Inputs**: initial guess $x_0$, learning rate $\\alpha \\in \\mathbb{R}^+$, total number of timesteps $T$. \n",
    "\n",
    "1. Set $t \\gets 0$. \n",
    "2. While $t < T$: \n",
    "    - Set $x_t \\gets x_{t-1} - \\alpha x_{t-1}$. \n",
    "    - $t \\gets t+1$.\n",
    "\n",
    "You can see this algorithm in action in the code block and plot above."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part A: Gradient Descent Converges (Sometimes)\n",
    "\n",
    "Let $\\delta_t = |x_t - 0| = |x_t|$ be the *error* in our current estimate. You can think of $\\delta_t$ as the distance between the correct answer (which is $x^* = 0$) and our current guess $x_t$. We'd like to make the error $\\delta_t$ close to 0, and we'd like to do so as quickly as possible. \n",
    "\n",
    "### Exercise A.1\n",
    "\n",
    "At the board with your group, write a careful calculation which proves the recursion relation\n",
    "\n",
    "$$\n",
    "\\delta_t = |1 - \\alpha | \\delta_{t-1}\\;.\n",
    "$$\n",
    "\n",
    "for the error $\\delta_t$ when using the gradient descent algorithm outlined above. Then, after lab, type your calculation and place it below. \n",
    "\n",
    "**Hints**: \n",
    "\n",
    "- $|a|$ is the absolute value of $a$.\n",
    "- For any $a, b \\in \\mathbb{R}$, $|ab| = |a||b|$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[*Type a careful version of your calculation below.*]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise A.2 \n",
    "\n",
    "At the board, write a proof using induction on the timestep $t$ to prove that the following explicit formula for $\\delta_t$ solves the recurrence relation from Exercise A.1:\n",
    "\n",
    "$$\n",
    "\\delta_t = |1-\\alpha|^{t} \\delta_0\\;. \n",
    "$$\n",
    "\n",
    "Then, after lab, type your proof and place it below. Please remember to include both a base case and an inductive step. Use Exercise A.1! "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[*Type a careful proof by induction based on your work.*]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The number $\\alpha$ is often called the *learning rate*. It's nice for the learning rate to be large, because this often allows us to get good approximations quickly. However, we can't choose this number to be too large...\n",
    "\n",
    "## Exercise A.3 \n",
    "\n",
    "There exist numbers $\\alpha_*$ and $\\alpha^*$ with the property that $\\delta_t < \\delta_{t-1}$ if and only if $\\alpha_* < \\alpha < \\alpha^*$. Determine the values of $\\alpha_*$ and $\\alpha^*$ and explain. You'll need to use the recursion relation in A.2. \n",
    "\n",
    "**Hint**: You may need to think back to ideas like *exponential growth* and *exponential decay*. "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[*Your solution here*]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise A.4\n",
    "\n",
    "Suppose that we do $1,000$ steps of gradient descent, so that $T = 1,000$. Consider the following three possibilities: \n",
    "\n",
    "1. $\\delta_T$ becomes very close to 0. \n",
    "2. $\\delta_T$ stays at its original value $\\delta_0$. \n",
    "3. $\\delta_T$ becomes very large. \n",
    "\n",
    "Describe which of these possibilities  will occur in the following scenarios: \n",
    "\n",
    "- Scenario I: $\\alpha = 0$. \n",
    "- Scenario II: $\\alpha = 3$. \n",
    "- Scenario III: $\\alpha = 1/2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[*Your response here*]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise A.5 \n",
    "\n",
    "Fill in the blanks and write a proof of the following theorem. \n",
    "\n",
    "**Theorem**: If we do many steps of gradient descent for the function $f(x) = \\frac{1}{2}x^2$ with learning rate $\\alpha$, the error $\\delta_t$ will converge (become very close) to 0 if and only if $\\_\\_\\_ < \\alpha < \\_\\_\\_$. \n",
    "\n",
    "**Hint**: *To prove the theorem, you should just cite your answers to the previous parts. You don't actually have to do very much new mathematics in this exercise; you just have to connect your results together in a full proof.* "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[*Your theorem statement and proof here*.]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise A.6\n",
    "\n",
    "Suppose that I start with an initial guess $x_0$. I would like to run my gradient descent algorithm long enough that, in my final timestep $t$, $\\delta_T < \\epsilon$ for some small number $\\epsilon > 0$. $\\epsilon$ is often called the *tolerance*. Using your response from Exercise A.2, solve for $T$ in terms of the tolerance $\\epsilon$ in order to obtain an expression for the number of timesteps required to achieve an error no larger than the tolerance. **Please show your work!** The result should be a formula for the necessary number of timesteps $T$ toin terms of $\\epsilon$. \n",
    "\n",
    "\n",
    "**Hint**: You will need to use logarithms to solve for $T$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[*Your calculation for the required number of steps $T$ to achieve a given tolerance*]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part B\n",
    "\n",
    "Below, I've started a computational experiment in which I've run the gradient descent algorithm for one value of the learning rate $\\alpha$. On the horizontal axis I've plotted the iteration number, and on the vertical axis I've plotted the error $\\delta_t$ on log-scaled axes. \n",
    "\n",
    "For the value of $\\alpha$ I chose, the error decreases on the log-scaled axes. \n",
    "\n",
    "Guided by your results in the previous parts, add two values of $\\alpha$ to the `alpha_vals` list such that: \n",
    "\n",
    "- In one case, the error *increases*. Bad! \n",
    "- In another case, the error is constant and never changes. Whoops! \n",
    "\n",
    "All you need to do is add two values to the `alpha_vals` list and then run the code cell. \n",
    "\n",
    "**Hint**: your solution should look like \n",
    "\n",
    "```python\n",
    "alpha_vals = [0.1, bad_alpha, constant_alpha]\n",
    "```\n",
    "\n",
    "for some values `bad_alpha` and `constant_alpha` that you determine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you only need to edit this list!\n",
    "\n",
    "\n",
    "alpha_vals = [0.1]\n",
    "\n",
    "# SOLUTION (lots of valid values), the plot should have a decreasing line, an increasing line, and a horizontal line\n",
    "\n",
    "\n",
    "## -------\n",
    "## no need to touch anything below\n",
    "## -------\n",
    "\n",
    "colors = [\"orange\", \"blue\", \"black\", \"grey\"]\n",
    "for i in range(len(alpha_vals)): # for each value of alpha\n",
    "    \n",
    "    # do a complete algorithm run\n",
    "    x_guess = 0.9\n",
    "    alpha = alpha_vals[i]\n",
    "    color = colors[i]\n",
    "    \n",
    "    T = 100\n",
    "    t = 0\n",
    "\n",
    "    # plot the results\n",
    "    plt.scatter([t], [x_guess], zorder = 10, s = 5, color = color, label = r\"$\\alpha = $\" + str(alpha))\n",
    "    while t < T: \n",
    "        plt.scatter([t], [x_guess], zorder = 10, s = 5, color = color)\n",
    "        x_guess = x_guess - alpha*x_guess\n",
    "        t = t+1\n",
    "\n",
    "# label the plot\n",
    "plt.semilogy()\n",
    "plt.legend()\n",
    "labs = plt.gca().set(xlabel = \"Iteration $t$\", ylabel = r\"Error $\\delta_t$ (log-scale)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part C (Required for **E**, optional for **M**)\n",
    "\n",
    "*If you are happy with an **M** on this assignment, you can skip all of Part C. You should solve this part only if you want an E.*"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Consider now the general quadratic function $f(x) = \\frac{1}{2} ax^2 + bx + c$ for some $a > 0$ and $b, c \\in \\mathbb{R}$. \n",
    "\n",
    "This function has its minimum at the point $x_* = -\\frac{b}{a}$. For this function, the gradient descent algorithm is just a little bit different:  \n",
    "\n",
    "**Inputs**: initial guess $x_0$, learning rate $\\alpha \\in \\mathbb{R}^+$, total number of timesteps $T$. \n",
    "\n",
    "1. Set $t \\gets 0$. \n",
    "2. While $t < T$: \n",
    "    - Set $x_t \\gets x_{t-1} - \\alpha (ax_{t-1} + b)$. \n",
    "    - $t \\gets t+1$.\n",
    "\n",
    "In this section, you'll repeat the exercises in Part A for this more general function. The results will be similar but a little more complicated. "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise C.1\n",
    "\n",
    "Define  $\\delta_t = |x_t - x_*|$. Prove the recursion relation \n",
    "\n",
    "$$\n",
    "\\delta_t = |1 - \\alpha a | \\delta_{t-1}\\;.\n",
    "$$\n",
    "\n",
    "*Hint*: the algebra here is messier but it's nothing more than grouping, rearranging fractions, and factoring. "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[*Your proof here*]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise C.2 \n",
    "\n",
    "There exist numbers $\\alpha_*$ and $\\alpha^*$ such that we are guaranteed that $\\delta_t \\rightarrow 0$ if any only if $\\alpha_* < \\alpha < \\alpha^*$. What are $\\alpha_*$ and $\\alpha^*$ in terms of $a$, $b$, and $c$? Justify your response. "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[*your response here*]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Exercise C.3 \n",
    "\n",
    "Suppose that indeed $\\alpha_* < \\alpha < \\alpha^*$. Suppose that we start with initial guess $x_0$. Suppose that we wish to run our algorithm for a number of timesteps $T$ so that the error $\\delta_T$ is guaranteed to be less than $\\epsilon$. Solve for $T$ in terms of $\\epsilon$, $x_0$, $a$, $b$, and $c$ to find the required number of steps. **Please show your work!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[*Your calculation here*]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part D: Reflection\n",
    "\n",
    "Write a paragraph in response to the following prompt: \n",
    "\n",
    "1. Approximately how long did it take you to complete this lab, **including Friday's in-class lab time**? \n",
    "2. Big picture, how would you describe what this lab was about?\n",
    "3. What are three skills you learned or practiced in this lab?\n",
    "4. What is a way in which your group supported your learning during this lab?\n",
    "5. What's something you found interesting, stimulating, or fun about this lab?\n",
    "6. What's something you found challenging, disappointing, or frustrating about this lab?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[*Your reflection here*]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hypergraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
